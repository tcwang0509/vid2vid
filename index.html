<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en"><head>
  <title>Video-to-Video Synthesis</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content="https://tcwang0509.github.io/vid2vid/images/teaser.gif"/>
<meta property="og:title" content="Video-to-Video Synthesis" />

<script src="lib.js" type="text/javascript"></script>
<script src="popup.js" type="text/javascript"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53682931-1', 'auto');
  ga('send', 'pageview');

</script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 0px;
	FLOAT: right;
	PADDING-BOTTOM: 0px;
	PADDING-TOP: 0px
}
#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1150? "1150px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1150px }
BODY {
	TEXT-ALIGN: center
}
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script src="b5m.js" id="b5mmain" type="text/javascript"></script></head>

<body>

<div id="primarycontent">
<center><h1>Video-to-Video Synthesis</h1></center>
<center><h2>
  <a href="https://tcwang0509.github.io/">Ting-Chun Wang</a><sup>1</sup>&nbsp;&nbsp;
  <a href="http://mingyuliu.net/">Ming-Yu Liu<sup>1</sup>&nbsp;&nbsp;
  <a href="http://people.csail.mit.edu/junyanz/">Jun-Yan Zhu</a><sup>2</sup>&nbsp;&nbsp;
  <a href="https://liuguilin1225.github.io/">Guilin Liu</a><sup>1</sup>&nbsp;&nbsp;
  <a href="https://www.linkedin.com/in/andrew-tao-6b6369/?trk=public-profile-join-page">Andrew Tao</a><sup>1</sup>&nbsp;&nbsp;
  <a href="http://jankautz.com/">Jan Kautz</a><sup>1</sup>&nbsp;&nbsp;
  <a href="http://catanzaro.name/">Bryan Catanzaro</a><sup>1</sup></center>
  
<center><h2><sup>1</sup>NVIDIA Corporation&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup>MIT</h2></center>

<center><h2><strong><a href="./paper_vid2vid.pdf">[Paper]</a>  
<a href="https://github.com/NVIDIA/vid2vid">[Code]</a></p>  
</strong> </h2></center>
<center><a href="images/teaser.gif">
<img style="PADDING-LEFT: 220px; PADDING-RIGHT: 220px;" src="images/teaser.gif" width="720"> </a></center>

<h2>Abstract</h2>
<div style="font-size:14px"><p>We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image synthesis problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without understanding temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a novel video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generator and discriminator architectures, coupled with a spatial-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines.  In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our approach to future video prediction, outperforming several state-of-the-art competing systems.</p></div>

<a href="https://arxiv.org/"><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="images/paper_thumbnail.jpg" width=170></a>
<br>

<h2>Paper</h2>
<p><a href="https://arxiv.org/">arXiv</a>,  2018. </p>



<h2>Citation</h2>
<p>Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. "Video-to-Video Synthesis", in arXiv, 2018.
<a href="Bibtex.txt">Bibtex</a>

</p>


<h2>Code: <a href="https://github.com/NVIDIA/vid2vid">Pytorch</a></h2>
<br>

<h1 align='center'>Our Example Results</h1>
<table align="center" border="0" cellspacing="0" cellpadding="0">
    <tr>
    <td align="center" valign="middle">
    <p> <iframe width="1024" height="575" src="https://www.youtube.com/embed/S1OwOd-war8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
   </td>
    </tr>
</table>
<p>&nbsp;</p>

<h1 align='center'>Semantic Labels &#8594 Cityscapes Street Views</h1>
<table border="0" cellspacing="10" cellpadding="0">
  <tr>
    <td align="center" valign="middle">
      <p> <iframe width="560" height="315" src="https://www.youtube.com/embed/A7g4mLD1E1E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
    </td>
    <td align="center" valign="middle">
      <p> <iframe width="560" height="315" src="https://www.youtube.com/embed/IHbnldNzapU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>  </p>
    </td>
  </tr>
  <!--<tr>
    <td align="center" valign="middle"><a href="images/city_change_styles.gif"><img src="images/city_change_styles.gif" width=560> </a></td>
    <td align="center" valign="middle"><a href="images/city_change_labels.gif"><img src="images/city_change_labels.gif" width=560> </a></td>
  </tr>-->
</table>
<p>&nbsp;</p>

<h1 align='center'>Edge &#8594 Face</h1>
<table border="0" cellspacing="10" cellpadding="0">
  <tr>
    <td align="center" valign="middle">
    <p> <iframe width="560" height="315" src="https://www.youtube.com/embed/J61Iutbjey4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
   </td>
   <td align="center" valign="middle">
    <p> <iframe width="560" height="315" src="https://www.youtube.com/embed/LivIo2mB-gA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
   </td>
    </tr>  
</table>
<p>&nbsp;</p>

<h1 align='center'>Pose &#8594 Body</h1>
<p> <div style="text-align: center;"><iframe width="560" height="315" src="https://www.youtube.com/embed/1_pLXQ_qXpQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div></p>

<br>
<h2>Acknowledgement</h2>
<p>We thank Karan Sapra, Fitsum Reda, and Matthieu Le for generating the segmentation maps for us. We also thank Lisa Rhee for allowing us to use her dance videos for training. We thank William S. Peebles for proofreading the paper.</p>

<h2>Citation</h2>
<p>If you find this useful for your research, please use the following.<br>
@article{wang2018vid2vid,<br>
  &emsp;&emsp;title={Video-to-Video Synthesis},<br>
  &emsp;&emsp;author={Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu and Guilin Liu and Andrew Tao and Jan Kautz and Bryan Catanzaro}, <br>
  &emsp;&emsp;journal={arXiv preprint}, <br>
  &emsp;&emsp;year={2018} <br>
}</p>

<div style="display:none">
<!-- GoStats JavaScript Based Code -->
<script type="text/javascript" src="http://gostats.com/js/counter.js"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script>
<noscript><a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"><img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" /></a></noscript>
</div>
<!-- End GoStats JavaScript Based Code -->
</body></html
>
